
# Vector Bookshelf in Semantic Kernel

This project is an essay to acquire a small collection of open-source
books and to make them available for sematic search and attribution,
using Microsoft's Semantic Kernel framework and locally-hosted
large-language models.

About 100 books in various domains were selected from the Project 
Gutenberg Archive as the right scale for an initial project.

## Questions to answer

Tools & Frameworks
  - How is Sematic Kernel used to create a document database?
  - How are documents acquired?
  - How is document chunking, vectorization and storage performed?

Document Processing & Structure
  - How can source attribution be maintained?
  - How are chunk sizes determined?
  - How do different embedding models affect search quality?
  - How can metadata (author, year, genre) be leveraged in searches?

Search Capabilities
  - What are some interesting question that can be posed to such a database?
  - How is cross-referencing between works accomplished?
  - How effective is multi-language support?

Performance & Scalability
  - What machine resources are required?
  - How do chunk sizes and search precision balance?
  - Can the system handle updates and additions efficiently?
  - How is the application to be deployed and shared?

User Experience
  - What query patterns are most effective for literary analysis?
  - How can search results be presented most meaningfully?
  - Can a web application interact with the document database?
